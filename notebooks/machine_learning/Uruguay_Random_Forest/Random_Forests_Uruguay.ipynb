{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-28T16:18:02.599748Z",
     "iopub.status.busy": "2020-09-28T16:18:02.598999Z",
     "iopub.status.idle": "2020-09-28T16:18:02.899031Z",
     "shell.execute_reply": "2020-09-28T16:18:02.899500Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.environ.get('NOTEBOOK_ROOT'))\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-28T16:18:02.902701Z",
     "iopub.status.busy": "2020-09-28T16:18:02.902274Z",
     "iopub.status.idle": "2020-09-28T16:18:03.117766Z",
     "shell.execute_reply": "2020-09-28T16:18:03.117234Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-28T16:18:03.122944Z",
     "iopub.status.busy": "2020-09-28T16:18:03.122245Z",
     "iopub.status.idle": "2020-09-28T16:18:03.137009Z",
     "shell.execute_reply": "2020-09-28T16:18:03.137440Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### See Label Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-28T16:18:03.142917Z",
     "iopub.status.busy": "2020-09-28T16:18:03.142434Z",
     "iopub.status.idle": "2020-09-28T16:18:03.145274Z",
     "shell.execute_reply": "2020-09-28T16:18:03.144828Z"
    }
   },
   "outputs": [],
   "source": [
    "df.groupby(\"LandUse\").size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-28T16:18:03.148879Z",
     "iopub.status.busy": "2020-09-28T16:18:03.148461Z",
     "iopub.status.idle": "2020-09-28T16:18:03.600309Z",
     "shell.execute_reply": "2020-09-28T16:18:03.600730Z"
    }
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot\n",
    "import seaborn as sns\n",
    "\n",
    "fig, ax = pyplot.subplots(figsize=(15,5))\n",
    "sns.countplot(x=\"LandUse\",data=df, palette=\"Greens_d\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Re-org\n",
    "\n",
    "The following classes do not have enough training examples to guarantee high enough variance:  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `Forestry`\n",
    "- `Fruittrees`\n",
    "- `Nativeforest`\n",
    "- `Stubble`\n",
    "- `Water`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My suggestion is reducing features/label-set to the following:  \n",
    "\n",
    "- `Natural Grassland`  \n",
    "- `Summercrops`  \n",
    "- `Prairie`  \n",
    "  \n",
    "as well as a grouping of `Forestry`, `FruitTrees`, and `Native Forest` under:  \n",
    "\n",
    "- `Forest`  \n",
    "\n",
    "then grouping other labels under a class called:\n",
    "\n",
    "- `Misc`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rebuild Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-28T16:18:03.604809Z",
     "iopub.status.busy": "2020-09-28T16:18:03.604388Z",
     "iopub.status.idle": "2020-09-28T16:18:03.606221Z",
     "shell.execute_reply": "2020-09-28T16:18:03.606549Z"
    }
   },
   "outputs": [],
   "source": [
    "def combine_classes_on_dataframe(_df, classes = [], combine_to = \"default\", column_name = \"LandUse\"):\n",
    "    df_new = df.copy()  \n",
    "    df_new[column_name].update(df_new[column_name].map(lambda x: combine_to if x in classes else x ))\n",
    "    return df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-28T16:18:03.612740Z",
     "iopub.status.busy": "2020-09-28T16:18:03.612303Z",
     "iopub.status.idle": "2020-09-28T16:18:03.614128Z",
     "shell.execute_reply": "2020-09-28T16:18:03.614549Z"
    }
   },
   "outputs": [],
   "source": [
    "df_new = df.copy()  \n",
    "df_new['LandUse'].update(df_new['LandUse'].map(lambda x: \"Forest\" if x in [\"Forestry\",\"Fruittrees\",\"Nativeforest\"] else x ))\n",
    "df_new['LandUse'].update(df_new['LandUse'].map(lambda x: \"Misc\" if x  not in [\"Forest\",\"Prairie\",\"Summercrops\",\"Naturalgrassland\"] else x ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-28T16:18:03.619473Z",
     "iopub.status.busy": "2020-09-28T16:18:03.618839Z",
     "iopub.status.idle": "2020-09-28T16:18:03.621256Z",
     "shell.execute_reply": "2020-09-28T16:18:03.621711Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_new.groupby(\"LandUse\").size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-28T16:18:03.636481Z",
     "iopub.status.busy": "2020-09-28T16:18:03.630097Z",
     "iopub.status.idle": "2020-09-28T16:18:03.756313Z",
     "shell.execute_reply": "2020-09-28T16:18:03.756757Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = pyplot.subplots(figsize=(15,5))\n",
    "sns.countplot(x=\"LandUse\",data=df_new, palette=\"Greens_d\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-28T16:18:03.760066Z",
     "iopub.status.busy": "2020-09-28T16:18:03.759632Z",
     "iopub.status.idle": "2020-09-28T16:18:04.653869Z",
     "shell.execute_reply": "2020-09-28T16:18:04.652714Z"
    }
   },
   "outputs": [],
   "source": [
    "import datacube\n",
    "dc = datacube.Datacube()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**LIsting products with 'uruguay' in its name**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc.list_products()[dc.list_products()['name'].str.contains(\"uruguay\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Load the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = dict(platform = 'LANDSAT_8',\n",
    "              product   = 'ls8_lasrc_uruguay',\n",
    "              latitude = (-34.44988376, -34.096445),\n",
    "              longitude = (-56.29119062, -55.24653668),\n",
    "              time = ('2016-01-01', '2017-01-01'), \n",
    "              measurements = ['red', 'green', 'blue', 'nir', 'swir1', 'swir2', 'pixel_qa'])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dc.load(**params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Composite Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import datetime \n",
    "from utils.data_cube_utilities.dc_mosaic import ls8_unpack_qa\n",
    "from utils.data_cube_utilities.dc_mosaic import create_median_mosaic\n",
    "\n",
    "def clean_mask_ls8(ds):\n",
    "    water_mask = ls8_unpack_qa(ds.pixel_qa, cover_type = \"water\")\n",
    "    clear_mask = ls8_unpack_qa(ds.pixel_qa, cover_type = \"clear\")\n",
    "    clean_mask = np.logical_or(water_mask, clear_mask)\n",
    "    return clean_mask\n",
    "\n",
    "def compress_on_time(_ds, date_range):\n",
    "    subset = _ds.sel(time = slice(*date_range)) # select a smaller date-range \n",
    "    \n",
    "    water_mask = ls8_unpack_qa(subset.pixel_qa, cover_type = \"water\")\n",
    "    clear_mask = ls8_unpack_qa(subset.pixel_qa, cover_type = \"clear\")\n",
    "    clean_mask = np.logical_or(water_mask, clear_mask)\n",
    "    \n",
    "    return create_median_mosaic(subset, clean_mask = clean_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_composite = compress_on_time(dataset, ('2016-01-01', '2017-01-01'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset_composite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Normalization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_composite = dataset_composite.drop('pixel_qa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_composite_normalalized = dataset_composite.copy(deep = True)\n",
    "for band_name in dataset_composite_normalalized.data_vars:  \n",
    "    arr = dataset_composite_normalalized[band_name].values\n",
    "    arr[arr > 10000] = 10000\n",
    "    arr[arr < 0] = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_composite_normalalized = dataset_composite_normalalized  * (1/10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_composite_normalalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default features for a Landsat based classifier are:\n",
    "- `red`\n",
    "- `green`\n",
    "- `blue`\n",
    "- `nir`\n",
    "- `swir1`\n",
    "- `swir1`  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "High order relationships between these features can also be used inputs to a random forest classifier. The following indices are already used in Landsat analysis and correlate with the existence of several land classes like water, chlorophyll, sediment/top-soil:  \n",
    "\n",
    "- `NDVI`\n",
    "- `NDVI Covariance`\n",
    "- `NDWI`\n",
    "- `NDBI`\n",
    "- `SCI`\n",
    "- `CCCI`\n",
    "- `CVI`\n",
    "- `PNDVI`\n",
    "\n",
    "These are not the best features to include but are easy to source. The code below should serve as a template of adding new features to the random forest feature classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">#### Plotting Utils  \n",
    ">Plotting utiltiies to help visualize features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_xarray(data_array):\n",
    "    fig = plt.figure(figsize = (15,5))\n",
    "    plt.axis('equal')\n",
    "    data_array.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_pixels_from_dataarray_using_pandas_array(ds,\n",
    "                                                    pandas_array = None,\n",
    "                                                    latitude_name = None,\n",
    "                                                    longitude_name = None,\n",
    "                                                    land_cover_name = None):\n",
    "    \n",
    "    landcover_names = list(pandas_array.groupby(land_cover_name).size().index)\n",
    "    training_dict   = {name: [] for name in landcover_names}\n",
    "    pandas_rows     = [x[1] for x in pandas_array.iterrows()]\n",
    "\n",
    "    for point_data in pandas_rows:\n",
    "        training_dict[point_data[land_cover_name]].append(\n",
    "            float(ds.sel(latitude  = point_data[latitude_name],\n",
    "                         longitude = point_data[longitude_name],\n",
    "                         method = 'nearest')))\n",
    "    data_list = []\n",
    "    target_list = []\n",
    "    ordered_class_names = []\n",
    "    for i, name in enumerate(training_dict.keys()):\n",
    "        ordered_class_names.append(name)\n",
    "        data_list += training_dict[name]\n",
    "        target_list += [i]*len(training_dict[name])\n",
    "\n",
    "    classes = list(map(lambda class_name_index: ordered_class_names[class_name_index], target_list))\n",
    "    \n",
    "    return pd.DataFrame({\"data\": data_list,\n",
    "                         \"landcover\": classes})\n",
    "\n",
    "from functools import partial    \n",
    "\n",
    "select_pixels = partial(select_pixels_from_dataarray_using_pandas_array,\n",
    "                        pandas_array   = df_new,\n",
    "                        latitude_name  = \"Latitude\",\n",
    "                        longitude_name = \"Longitude\",\n",
    "                        land_cover_name = \"LandUse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns  \n",
    "\n",
    "def jitter_plot(ds, name):\n",
    "    data = select_pixels(ds)\n",
    "    data = data.rename(index = str, columns={\"data\":name})\n",
    "    plt.figure(figsize = (15,5))\n",
    "    ax = sns.stripplot(x=name, y=\"landcover\", data=data , jitter=True, linewidth=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **NDVI**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NDVI(dataset):\n",
    "    return (dataset.nir - dataset.red)/(dataset.nir + dataset.red).rename(\"NDVI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_xarray( NDVI(dataset_composite) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jitter_plot(NDVI(dataset_composite), name = \"NDVI\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **NDVI Covariance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr  \n",
    "\n",
    "def covariance(da:xr.DataArray):\n",
    "    return da.std(dim = \"time\")/da.mean(dim = \"time\") \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def covariance_NDVI(ds, mask = None):\n",
    "    ## TODO: Lift masking out of function \n",
    "#     mask = np.ones(ds[list(ds.data_vars.keys())[0]].values.shape).astype(bool) if mask is None else mask\n",
    "    ds_ndvi = NDVI(ds)    \n",
    "    ds_ndvi = ds_ndvi.where(mask)\n",
    "    return covariance(ds_ndvi)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "def finite_histogram(data_array, *args, **kwargs):\n",
    "    x = data_array.values.copy()\n",
    "    x = x[~np.isnan(x)]\n",
    "    x = x[np.isfinite(x)]\n",
    "    \n",
    "    data = plt.hist(x,*args, **kwargs)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndvi_cov = covariance_NDVI(dataset,\n",
    "                           mask = clean_mask_ls8(dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *NDVI coefficient of variance histogram*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (15,5))\n",
    "plt.imshow(ndvi_cov.values,\n",
    "           vmin=0,\n",
    "           vmax=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (15,5))\n",
    "finite_histogram(ndvi_cov, bins = 2000, range = (0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jitter_plot(ndvi_cov, name = \"coefficient_of_variance_NDVI\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **NBR**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NBR(dataset):\n",
    "    return ((dataset.nir - dataset.swir2) / (dataset.swir2 + dataset.nir)).rename(\"NBR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_xarray(  NBR(dataset_composite)  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jitter_plot( NBR(dataset_composite), name = \"NBR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **NDBI**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NDBI(dataset):\n",
    "    return (( dataset.swir1 - dataset.nir ) / (dataset.swir1 + dataset.nir)).rename(\"NDBI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_xarray( NDBI(dataset_composite) ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jitter_plot( NDBI(dataset_composite), name = \"NDBI\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**NDWI**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NDWI_1(dataset):\n",
    "    return (dataset.nir - dataset.swir1)/(dataset.nir + dataset.swir1).rename(\"NDWI_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_xarray( NDWI_1(dataset_composite) ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jitter_plot( NDWI_1(dataset_composite), name = \"NDWI_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NDWI_2(dataset):\n",
    "    return (dataset.green - dataset.nir)/(dataset.green + dataset.nir).rename(\"NDWI_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_xarray( NDWI_2(dataset_composite) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jitter_plot( NDWI_2(dataset_composite), name = \"NDWI_2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**SCI**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SCI(dataset):\n",
    "    return ((dataset.swir1 - dataset.nir)/(dataset.swir1 + dataset.nir)).rename(\"SCI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_xarray( SCI(dataset_composite) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jitter_plot( SCI(dataset_composite), name = \"SCI\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **NBR2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NBR2(dataset):\n",
    "    return (dataset.swir1 - dataset.swir2)/(dataset.swir1 + dataset.swir2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_xarray( NBR2(dataset_composite) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jitter_plot( NBR2(dataset_composite), name = \"NBR2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **CCCI**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CCCI(dataset):\n",
    "    return ((dataset.nir - dataset.red)/(dataset.nir + dataset.red)).rename(\"CCCI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_xarray( CCCI(dataset_composite) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jitter_plot( CCCI(dataset_composite), name = \"CCCI\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **CVI**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CVI(dataset):\n",
    "    return (dataset.nir * (dataset.red / (dataset.green * dataset.green))).rename(\"CVI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_xarray( CVI(dataset_composite) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jitter_plot( CVI(dataset_composite), name = \"CVI\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **PNDVI**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PNDVI(dataset):\n",
    "    \n",
    "    nir = dataset.nir\n",
    "    green = dataset.green\n",
    "    blue = dataset.blue\n",
    "    red = dataset.red\n",
    "    \n",
    "    return ((nir - (green + red + blue))/(nir + (green + red + blue))).rename(\"PNDVI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_xarray( PNDVI(dataset_composite) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "jitter_plot( PNDVI(dataset_composite), name = \"PNDVI\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### Spectral Indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for key in dataset_composite.data_vars:\n",
    "    jitter_plot( dataset_composite[key], name = key)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Normalize and merge features into a single data-set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr  \n",
    "def normalize( da :xr.DataArray) -> xr.Dataset:\n",
    "    return (da-da.min())/(da.max() - da.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pndvi  = PNDVI(  dataset_composite).to_dataset(name = \"PNDVI\")\n",
    "ndbi   = NDBI(   dataset_composite).to_dataset(name = \"NDBI\")\n",
    "nbr    = NBR(    dataset_composite).to_dataset(name = \"NBR\")\n",
    "nbr2   = NBR2(   dataset_composite).to_dataset(name = \"NBR2\")\n",
    "cvi    = CVI(    dataset_composite).to_dataset(name = \"CVI\")\n",
    "ccci   = CCCI(   dataset_composite).to_dataset(name = \"CCCI\")\n",
    "sci    = SCI(    dataset_composite).to_dataset(name = \"SCI\")\n",
    "ndwi1  = NDWI_1( dataset_composite).to_dataset(name = \"NDWI1\")\n",
    "ndwi2  = NDWI_2( dataset_composite).to_dataset(name = \"NDWI2\")\n",
    "ndvi   = NDVI(   dataset_composite).to_dataset(name = \"NDVI\")\n",
    "cov_ndvi = ndvi_cov.to_dataset(name = \"NDVI_Covariance\")\n",
    "features = xr.merge([normalize(pndvi),\n",
    "                     normalize(cvi),\n",
    "                     normalize(nbr),\n",
    "                     normalize(nbr2),\n",
    "                     normalize(ndbi),\n",
    "                     normalize(ccci),\n",
    "                     normalize(sci),\n",
    "                     normalize(ndwi1),\n",
    "                     normalize(ndwi2),\n",
    "                     normalize(ndvi),\n",
    "                     cov_ndvi,\n",
    "                     dataset_composite_normalalized]\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.red.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>  \n",
    "## Build Training Data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section involves formatting the data for random forest classifier training. The approach is not well polished and may un-necessarily complicate the process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "landcover_names = list(df_new.groupby(\"LandUse\").size().index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dict = {name: [] for name in landcover_names}\n",
    "training_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_rows = [x[1] for x in df_new.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Example of first 2 elements\n",
    "pandas_rows[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xarray_pixel_to_array(_dataset):\n",
    "    return [float(_dataset[variable_name].values) for variable_name in _dataset.data_vars]\n",
    "\n",
    "for point_data in pandas_rows:\n",
    "    training_dict[point_data[\"LandUse\"]].append(\n",
    "        xarray_pixel_to_array(\n",
    "            features.sel(latitude = point_data[\"Latitude\"],\n",
    "                         longitude = point_data[\"Longitude\"],\n",
    "                         method = 'nearest')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = []\n",
    "target_list = []\n",
    "ordered_class_names = []\n",
    "for i, name in enumerate(training_dict.keys()):\n",
    "    ordered_class_names.append(name)\n",
    "    data_list += training_dict[name]\n",
    "    target_list += [i]*len(training_dict[name])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>  \n",
    "# k-fold cross validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentage_split = .33\n",
    "random_number_seed = 44"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "in_train, in_test, out_train, out_test = train_test_split(data_list,\n",
    "                                                          target_list,\n",
    "                                                          test_size = percentage_split,\n",
    "                                                          random_state = random_number_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Examine training-data distribution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (15,3))\n",
    "plt.hist(out_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Examine testing-data distribution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (15,3))\n",
    "plt.hist(out_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "# training a random forest classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(max_depth=10,\n",
    "                             n_estimators = 1000,\n",
    "                             random_state=random_number_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(in_train, out_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.score(in_test,\n",
    "          out_test,\n",
    "          sample_weight=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### linear search for optimum depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depth = 40\n",
    "random_number_seed = 43"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time \n",
    "\n",
    "hyper_param_list = []\n",
    "for i in range(2, max_depth):\n",
    "    t1 = time()\n",
    "    clf = RandomForestClassifier(max_depth=i,\n",
    "                                 n_estimators = 1000,\n",
    "                                 random_state=random_number_seed)\n",
    "    clf.fit(in_train, out_train)\n",
    "    t2 = time()\n",
    "    hyper_param_list.append( [i,clf.score(in_test, out_test)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = zip(*hyper_param_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (15,5))\n",
    "plt.plot(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(hyper_param_list, key = lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_depth = max(hyper_param_list, key = lambda x: x[1])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Rebuild Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(max_depth=18,\n",
    "                                 n_estimators = 1000,\n",
    "                                 random_state=random_number_seed)\n",
    "\n",
    "clf.fit(in_train, out_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Confusion Matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "y_true = out_test\n",
    "y_pred = clf.predict(in_test)\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **F1 Score**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = np.array(out_test)\n",
    "y_pred = np.array(clf.predict(in_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(tp,tn,fp,fn):\n",
    "    _precision = precision(tp, fp)\n",
    "    _recall    = recall(tp,fn)\n",
    "    return 2*(_recall * _precision) / (_recall + _precision)\n",
    "    \n",
    "def precision(tp, fp):\n",
    "    return tp/(tp + fp)\n",
    "\n",
    "def recall(tp, fn):\n",
    "    return tp / (tp + fn)\n",
    "\n",
    "def evaluate(_x,_y, class_name):\n",
    "    x = np.array(_x)\n",
    "    y = np.array(_y)\n",
    "    \n",
    "    tp_mask = np.logical_and((x == class_name) , (y == class_name))\n",
    "    fp_mask = np.logical_and((x == class_name) , (y != class_name))\n",
    "    \n",
    "    tn_mask = np.logical_and((x != class_name) , (y != class_name))\n",
    "    fn_mask = np.logical_and((x != class_name) , (y == class_name))\n",
    "    \n",
    "    tp = sum(tp_mask)\n",
    "    fp = sum(fp_mask)\n",
    "    \n",
    "    tn = sum(tn_mask)\n",
    "    fn = sum(fn_mask)\n",
    "    \n",
    "    return tp, fp, tn, fn\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, label in enumerate(ordered_class_names):\n",
    "    tp, fp, tn, fn = evaluate(y_pred, y_true, index)\n",
    "    print(\"F1 score {} :      {}\".format(label, f1(tp,tn,fp,fn)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Plotting the Confusion Matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "def plot_confusion_matrix(cm,\n",
    "                          target_names,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=None,\n",
    "                          normalize=True):\n",
    "    \"\"\"\n",
    "    Citiation\n",
    "    ---------\n",
    "    http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "    \"\"\"\n",
    "    \n",
    "    accuracy = np.trace(cm) / float(np.sum(cm))\n",
    "    misclass = 1 - accuracy\n",
    "\n",
    "    if cmap is None:\n",
    "        cmap = plt.get_cmap('Blues')\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    title = title if normalize == False else title + \" (Normalized)\"\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "\n",
    "    if target_names is not None:\n",
    "        tick_marks = np.arange(len(target_names))\n",
    "        plt.xticks(tick_marks, target_names, rotation=45)\n",
    "        plt.yticks(tick_marks, target_names)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "\n",
    "    thresh = cm.max() / 1.125 if normalize else cm.max() / 2\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        if normalize:\n",
    "            plt.text(j, i, \"{:0.2f}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"red\" if cm[i, j] > thresh else \"black\")\n",
    "        else:\n",
    "            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_confusion_matrix(cm, ordered_class_names, title='Uruguay Confusion Matrix', cmap=plt.cm.Blues, normalize=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_confusion_matrix(cm, ordered_class_names, title='Uruguay Confusion Matrix', cmap=plt.cm.Blues, normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Classifier Sensitivity **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sensitivity(_cm, labels):\n",
    "    _cm = np.array(_cm).astype(np.float64)\n",
    "    totals = np.sum(_cm, axis = 0)\n",
    "    diag_percentage = [cm[i][i]/total for i,total in enumerate(totals)]\n",
    "    replace_nans = lambda x: 0 if np.isnan(x) else x  \n",
    "    diag_percentage = map(replace_nans, diag_percentage)\n",
    "    return { a:b for a,b in zip(labels, diag_percentage) }\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensitivity(cm, ordered_class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>  \n",
    "> **Classifier Precision **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(_cm, labels):\n",
    "    _cm = np.array(_cm).astype(np.float64)\n",
    "    totals = np.sum(_cm, axis = 1)\n",
    "    diag_percentage = [cm[i][i]/total for i,total in enumerate(totals)]\n",
    "    replace_nans = lambda x: 0 if np.isnan(x) else x  \n",
    "    diag_percentage = map(replace_nans, diag_percentage)\n",
    "    return { a:b for a,b in zip(labels, diag_percentage) }\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision(cm, ordered_class_names)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
