{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Water Interoperability Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "There are a few water classifiers for Landsat, Sentinel-1, and Sentinel-2. We will examine WOfS for Landsat, thresholding for Sentinel-1, and WOfS for Sentinel-2.\n",
    "\n",
    "Although WOfS performs well on clear water bodies, it can misclassify murky water bodies as not water. WASARD or Sentinel-1 thresholding generally perform equally well or better than WOfS â€“ especially on murky water bodies.\n",
    "\n",
    "Because WOfS uses an optical data source (Landsat), it often does not have data to make water classifications due to cloud occlusion. The same limitation applies to Sentinel-2 water detection.\n",
    "\n",
    "The main reasons to use multiple data sources in the same water detection analysis are to increase temporal resolution and account for missing data.\n",
    "\n",
    "## Description\n",
    "\n",
    "This notebook checks how similar water classifications are among a selected set of sources (e.g. WOfS for Landsat, thresholding for Sentinel-1, etc.).\n",
    "These are the steps followed:\n",
    "\n",
    "1. Determine the dates of coincidence of data for the selected sensors using the CEOS COVE tool.\n",
    "1. Acquire water classifications for each sensor.\n",
    "1. Show the RGB representation of Time Slices and Water Classifications\n",
    "1. Obtain the intersected clean mask for the sensors.\n",
    "1. Show the per-time-slice percent of cloud according to each sensor as a line plot.\n",
    "1. Show the per-time-slice percent of water (masked with the intersected clean mask) according to each sensor as a line plot.\n",
    "1. Show the per-time-slice similarity (% of matching pixels) of each pair of sensors as a line plot.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting started\n",
    "\n",
    "**To run this analysis**, run all the cells in the notebook, starting with the \"Load packages\" cell.\n",
    "\n",
    "**After finishing the analysis**, return to the \"Analysis parameters\" cell, modify some values (e.g. choose a different location or time period to analyse) and re-run the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load packages\n",
    "Load key Python packages and supporting functions for the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-28T14:42:09.954999Z",
     "iopub.status.busy": "2020-09-28T14:42:09.954572Z",
     "iopub.status.idle": "2020-09-28T14:42:11.092452Z",
     "shell.execute_reply": "2020-09-28T14:42:11.092925Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "import datacube\n",
    "import numpy\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "from xarray.ufuncs import isnan as xr_nan\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to the datacube\n",
    "Activate the datacube database, which provides functionality for loading and displaying stored Earth observation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-28T14:42:11.097342Z",
     "iopub.status.busy": "2020-09-28T14:42:11.096650Z",
     "iopub.status.idle": "2020-09-28T14:42:11.407359Z",
     "shell.execute_reply": "2020-09-28T14:42:11.407825Z"
    }
   },
   "outputs": [],
   "source": [
    "dc = datacube.Datacube(app=\"water_interoperability_similarity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis parameters\n",
    "\n",
    "The following cell sets the parameters, which define the area of interest and the length of time to conduct the analysis over.\n",
    "The parameters are\n",
    "\n",
    "* `latitude`: The latitude range to analyse (e.g. `(-11.288, -11.086)`).\n",
    "For reasonable loading times, make sure the range spans less than ~0.1 degrees.\n",
    "* `longitude`: The longitude range to analyse (e.g. `(130.324, 130.453)`).\n",
    "For reasonable loading times, make sure the range spans less than ~0.1 degrees.\n",
    "\n",
    "**If running the notebook for the first time**, keep the default settings below.\n",
    "This will demonstrate how the analysis works and provide meaningful results.\n",
    "The example covers an area around Obuasi, Ghana.\n",
    "\n",
    "**To run the notebook for a different area**, make sure Landsat 8, Sentinel-1, and Sentinel-2 data is available for the chosen area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-28T14:42:11.411591Z",
     "iopub.status.busy": "2020-09-28T14:42:11.411071Z",
     "iopub.status.idle": "2020-09-28T14:42:11.412777Z",
     "shell.execute_reply": "2020-09-28T14:42:11.413191Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define the area of interest\n",
    "# Obuasi, Ghana\n",
    "# latitude = (6.10, 6.26)\n",
    "# longitude = (-1.82, -1.66)\n",
    "# latitude = (6.1582, 6.2028)\n",
    "# longitude = (-1.7295, -1.6914)\n",
    "\n",
    "# DEBUG - small area of Obuasi for quick loading\n",
    "# latitude = (6.1982, 6.2028)\n",
    "# longitude = (-1.7295, -1.6914)\n",
    "\n",
    "# Tono Dam, Ghana\n",
    "latitude = (10.8600, 10.9150) \n",
    "longitude = (-1.1850, -1.1425)\n",
    "\n",
    "# The time range in which we want to determine \n",
    "# dates of close scenes among sensors.\n",
    "time_extents = ('2014-01-01', '2018-12-31')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-28T14:42:11.416540Z",
     "iopub.status.busy": "2020-09-28T14:42:11.415849Z",
     "iopub.status.idle": "2020-09-28T14:42:11.508063Z",
     "shell.execute_reply": "2020-09-28T14:42:11.507200Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-a647680817d0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_cube_utilities\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdc_display_map\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdisplay_map\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdisplay_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlongitude\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlatitude\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'utils'"
     ]
    }
   ],
   "source": [
    "from utils.data_cube_utilities.dc_display_map import display_map\n",
    "\n",
    "display_map(longitude, latitude)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determine the dates of coincidence of data for the selected sensors using the COVE tool."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used a tool from the Committee on Earth Observations (CEOS) called the CEOS Visualization Environment (COVE). This tool has several applications, such as Acquisition Forecaster predicts when and where future acquisitions (images) will occur, and Coverage Analyzer which shows when and where acquisitions have occurred in the past.\n",
    "\n",
    "For this analysis, we used the Coincident Calculator to determine when Landsat 8, Sentinel-1, and Sentinel-2 have close dates so we can compare them on a per-time-slice basis.\n",
    "\n",
    "The [COVE Coincident Calculator](https://ceos-cove.org/en/coincident_calculator/) allows users to specify the sensors to determine coincidence for. For this analysis, we first determined the dates of coincidence of Landsat 8 and Sentinel-2. We then determined dates which are close to those which have Sentinel-1 data.\n",
    "\n",
    "We first found dates for which both Landsat 8 and Sentinel-2 data is available for the time range and area of interest, which were the following 8 dates:\n",
    "**\\[April 22, 2017, July 11, 2017, September 29, 2017, December 18, 2017, March 8, 2018, May 27, 2018, August 15, 2018, November 3, 2018\\]**\n",
    "\n",
    "Then we found dates for which Landsat 8 and Sentinel-1 data is available for the time range and area of interest, and then found the subset of closely matching dates, which were the following 6 dates: **\\[July 12, 2017 (off 1), September 29, 2017, December 15, 2017 (off 3), March 9, 2018 (off 1), May 27, 2018, August 12, 2018 (off 3)\\]** These are the daets we use in this analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acquire water classifications for each sensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_load_params = \\\n",
    "    dict(latitude=latitude, longitude=longitude, \n",
    "         group_by='solar_day', \n",
    "         output_crs=\"epsg:4326\",\n",
    "         resolution=(-0.00027,0.00027),\n",
    "         dask_chunks={'latitude': 2000, 'longitude':2000, 'time':1})\n",
    "\n",
    "# The minimum percent of data that a time slice must have\n",
    "# to be kept in this analysis\n",
    "MIN_PCT_DATA = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determine the time range of overlapping data for all sensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata['Landsat 8'] = \\\n",
    "    dc.load(**common_load_params,\n",
    "            product='ls8_lasrc_ghana', \n",
    "            time=time_extents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata['Sentinel-1'] = \\\n",
    "    dc.load(**common_load_params,\n",
    "            product='s1monthly_gamma0_ghana', \n",
    "            time=time_extents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No products match search terms: {'time': Range(begin=datetime.datetime(2014, 1, 1, 0, 0, tzinfo=<UTC>), end=datetime.datetime(2018, 12, 31, 23, 59, 59, 999999, tzinfo=tzutc())), 'lat': Range(begin=10.86, end=10.915), 'lon': Range(begin=-1.185, end=-1.1425), 'product': 's2a_msil2a'}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-429d96d34503>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m s2a_meta = dc.load(**common_load_params,\n\u001b[1;32m      2\u001b[0m                    \u001b[0mproduct\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m's2a_msil2a'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m                    time=time_extents)\n\u001b[0m\u001b[1;32m      4\u001b[0m s2b_meta = dc.load(**common_load_params,\n\u001b[1;32m      5\u001b[0m                    \u001b[0mproduct\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m's2b_msil2a'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/env/lib/python3.6/site-packages/datacube/api/core.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, product, measurements, output_crs, resolution, resampling, skip_broken_datasets, dask_chunks, like, fuse_func, align, datasets, progress_cbk, **query)\u001b[0m\n\u001b[1;32m    290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdatasets\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m             \u001b[0mdatasets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_datasets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproduct\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproduct\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlike\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlike\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0mdatasets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/env/lib/python3.6/site-packages/datacube/api/core.py\u001b[0m in \u001b[0;36mfind_datasets\u001b[0;34m(self, **search_terms)\u001b[0m\n\u001b[1;32m    329\u001b[0m         \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mseealso\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mmeth\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mgroup_datasets\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mmeth\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mload_data\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mmeth\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mfind_datasets_lazy\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m         \"\"\"\n\u001b[0;32m--> 331\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_datasets_lazy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0msearch_terms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfind_datasets_lazy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/env/lib/python3.6/site-packages/datacube/api/core.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mensure_location\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m             \u001b[0mdatasets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdatasets\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muris\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    357\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/env/lib/python3.6/site-packages/datacube/api/core.py\u001b[0m in \u001b[0;36mselect_datasets_inside_polygon\u001b[0;34m(datasets, polygon)\u001b[0m\n\u001b[1;32m    681\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mpolygon\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    682\u001b[0m     \u001b[0mquery_crs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolygon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcrs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 683\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    684\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mintersects\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolygon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_crs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_crs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    685\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/env/lib/python3.6/site-packages/datacube/index/_datasets.py\u001b[0m in \u001b[0;36msearch\u001b[0;34m(self, limit, **query)\u001b[0m\n\u001b[1;32m    510\u001b[0m         for product, datasets in self._do_search_by_product(query,\n\u001b[1;32m    511\u001b[0m                                                             \u001b[0msource_filter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msource_filter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 512\u001b[0;31m                                                             limit=limit):\n\u001b[0m\u001b[1;32m    513\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproduct\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/env/lib/python3.6/site-packages/datacube/index/_datasets.py\u001b[0m in \u001b[0;36m_do_search_by_product\u001b[0;34m(self, query, return_fields, select_field_names, with_source_ids, source_filter, limit)\u001b[0m\n\u001b[1;32m    633\u001b[0m         \u001b[0mproduct_queries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_product_queries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mproduct_queries\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 635\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'No products match search terms: %r'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproduct\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mproduct_queries\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: No products match search terms: {'time': Range(begin=datetime.datetime(2014, 1, 1, 0, 0, tzinfo=<UTC>), end=datetime.datetime(2018, 12, 31, 23, 59, 59, 999999, tzinfo=tzutc())), 'lat': Range(begin=10.86, end=10.915), 'lon': Range(begin=-1.185, end=-1.1425), 'product': 's2a_msil2a'}"
     ]
    }
   ],
   "source": [
    "s2a_meta = dc.load(**common_load_params,\n",
    "                   product='s2a_msil2a', \n",
    "                   time=time_extents)\n",
    "s2b_meta = dc.load(**common_load_params,\n",
    "                   product='s2b_msil2a', \n",
    "                   time=time_extents)\n",
    "metadata['Sentinel-2'] = xr.concat((s2a_meta, s2b_meta), dim='time').sortby('time')\n",
    "del s2a_meta, s2b_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Sentinel-2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-2b047a3d6677>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mls8_time_rng\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Landsat 8'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0ms2_time_rng\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Sentinel-2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtime_rng\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mls8_time_rng\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms2_time_rng\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0moverlapping_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime_rng\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime_rng\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Sentinel-2'"
     ]
    }
   ],
   "source": [
    "ls8_time_rng = metadata['Landsat 8'].time.values[[0,-1]]\n",
    "s2_time_rng = metadata['Sentinel-2'].time.values[[0,-1]]\n",
    "\n",
    "time_rng = np.stack((ls8_time_rng, s2_time_rng))\n",
    "overlapping_time = time_rng[:,0].max(), time_rng[:,1].min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Limit the metadata to check for close scenes to the overlapping time range.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sensor in metadata:\n",
    "    metadata[sensor] = metadata[sensor].sel(time=slice(*overlapping_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determine the dates of close scenes among the sensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants #\n",
    "# The maximum number of days of difference between scenes\n",
    "# from sensors for those scenes to be considered approximately coincident.\n",
    "# The Sentinel-1 max date diff is set high enough to allow any set of dates \n",
    "# from the other sensors to match with one of its dates since we will \n",
    "# select its matching dates with special logic later.\n",
    "MAX_NUM_DAYS_DIFF = {'Landsat 8':4, 'Sentinel-1':30}#, 'Sentinel-2':4}\n",
    "# End Constants #\n",
    "\n",
    "# all_times\n",
    "num_datasets = len(metadata)\n",
    "ds_names = list(metadata.keys())\n",
    "first_ds_name = ds_names[0]\n",
    "# All times for each dataset.\n",
    "ds_times = {ds_name: metadata[ds_name].time.values for ds_name in ds_names}\n",
    "# The time indices for each dataset's sorted time dimension \n",
    "# currently being compared.\n",
    "time_inds = {ds_name: 0 for ds_name in ds_names}\n",
    "corresponding_times = {ds_name: [] for ds_name in ds_names}\n",
    "\n",
    "# The index of the dataset in `metadata` to compare times against the first.\n",
    "oth_ds_ind = 1\n",
    "oth_ds_name = ds_names[oth_ds_ind]\n",
    "oth_ds_time_ind = time_inds[oth_ds_name]\n",
    "# For each time in the first dataset, find any \n",
    "# closely matching dates in the other datasets.\n",
    "for first_ds_time_ind, first_ds_time in enumerate(ds_times[first_ds_name]):\n",
    "    time_inds[first_ds_name] = first_ds_time_ind\n",
    "    # Find a corresponding time in this other dataset.\n",
    "    while True:\n",
    "        oth_ds_name = ds_names[oth_ds_ind]\n",
    "        oth_ds_time_ind = time_inds[oth_ds_name]\n",
    "        # If we've checked all dates for the other dataset, \n",
    "        # check the next first dataset time.\n",
    "        if oth_ds_time_ind == len(ds_times[oth_ds_name]):\n",
    "            break\n",
    "        oth_ds_time = metadata[ds_names[oth_ds_ind]].time.values[oth_ds_time_ind]\n",
    "        time_diff = (oth_ds_time - first_ds_time).astype('timedelta64[D]').astype(int)\n",
    "        \n",
    "        # If this other dataset time is too long before this\n",
    "        # first dataset time, check the next other dataset time.\n",
    "        if time_diff <= -MAX_NUM_DAYS_DIFF[oth_ds_name]:\n",
    "            oth_ds_time_ind += 1\n",
    "            time_inds[ds_names[oth_ds_ind]] = oth_ds_time_ind\n",
    "            continue\n",
    "        # If this other dataset time is within the acceptable range\n",
    "        # of the first dataset time...\n",
    "        elif abs(time_diff) <= MAX_NUM_DAYS_DIFF[oth_ds_name]:\n",
    "            # If there are more datasets to find a corresponding date for\n",
    "            # these current corresponding dates, check those datasets.\n",
    "            if oth_ds_ind < len(ds_names)-1:\n",
    "                oth_ds_ind += 1\n",
    "                continue\n",
    "            else: # Otherwise, record this set of corresponding dates.\n",
    "                for ds_name in ds_names:\n",
    "                    corresponding_times[ds_name].append(ds_times[ds_name][time_inds[ds_name]])\n",
    "                    # Don't use these times again.\n",
    "                    time_inds[ds_name] = time_inds[ds_name] + 1\n",
    "                oth_ds_ind = 1\n",
    "                break\n",
    "        # If this other dataset time is too long after this\n",
    "        # first dataset time, go to the next first dataset time.\n",
    "        else:\n",
    "            oth_ds_ind -= 1\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to pandas datetime\n",
    "for sensor in corresponding_times:\n",
    "    for ind in range(len(corresponding_times[sensor])):\n",
    "        corresponding_times[sensor][ind] = \\\n",
    "            pd.to_datetime(corresponding_times[sensor][ind])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The Sentinel-1 data is a monthly composite, so we need special logic for choosing data from it.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls8_pd_datetimes = corresponding_times['Landsat 8'] \n",
    "s1_pd_datetimes = pd.to_datetime(metadata['Sentinel-1'].time.values)\n",
    "for time_ind, ls8_time in enumerate(ls8_pd_datetimes):\n",
    "    matching_s1_time_ind = [s1_time_ind for (s1_time_ind, s1_time) \n",
    "                            in enumerate(s1_pd_datetimes) if \n",
    "                            s1_time.month == ls8_time.month][0]\n",
    "    matching_s1_time = metadata['Sentinel-1'].time.values[matching_s1_time_ind]\n",
    "    corresponding_times['Sentinel-1'][time_ind] = pd.to_datetime(matching_s1_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Landsat 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls8_times = corresponding_times['Landsat 8']\n",
    "s1_times = corresponding_times['Sentinel-1']\n",
    "s2_times = corresponding_times['Sentinel-2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls8_data = []\n",
    "ls8_data = dc.load(**common_load_params,\n",
    "                   product='ls8_usgs_sr_scene', \n",
    "                   time=(ls8_times[0], ls8_times[-1]),\n",
    "                   dask_chunks = {'time': 1})\n",
    "ls8_data = ls8_data.sel(time=corresponding_times['Landsat 8'], method='nearest')\n",
    "print(f\"Subset the data to {len(ls8_data.time)} times of near coincidence.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Acquire the clean mask**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from water_interoperability_utils.clean_mask import ls8_unpack_qa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls8_data_mask = (ls8_data != -9999).to_array().any('variable')\n",
    "ls8_clear_mask = ls8_unpack_qa(ls8_data.pixel_qa, 'clear')\n",
    "ls8_water_mask = ls8_unpack_qa(ls8_data.pixel_qa, 'water')\n",
    "ls8_clean_mask = (ls8_clear_mask | ls8_water_mask) & ls8_data_mask \n",
    "del ls8_clear_mask, ls8_water_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Acquire water classifications**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from water_interoperability_utils.dc_water_classifier import wofs_classify\n",
    "import warnings\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"ignore\", category=Warning)\n",
    "    ls8_water = wofs_classify(ls8_data).wofs\n",
    "ls8_water = ls8_water.where(ls8_clean_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentinel-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1_data = dc.load(**common_load_params,\n",
    "                  product='sentinel1_ghana_monthly', \n",
    "                  time=(s1_times[0], s1_times[-1]),\n",
    "                  dask_chunks = {'time': 1})\n",
    "s1_data = s1_data.sel(time=corresponding_times['Sentinel-1'], method='nearest')\n",
    "print(f\"Subset the data to {len(s1_data.time)} times of near coincidence.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Acquire the clean mask**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1_not_nan_da = ~xr_nan(s1_data).to_array()\n",
    "s1_clean_mask = s1_not_nan_da.min('variable')\n",
    "del s1_not_nan_da"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Acquire water classifications**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from skimage.filters import try_all_threshold, threshold_otsu\n",
    "\n",
    "thresh_vv = threshold_otsu(s1_data.vv.values)\n",
    "thresh_vh = threshold_otsu(s1_data.vh.values)\n",
    "\n",
    "binary_vv = s1_data.vv.values < thresh_vv\n",
    "binary_vh = s1_data.vh.values < thresh_vh\n",
    "\n",
    "s1_water = xr.DataArray(binary_vv & binary_vh, coords=s1_data.vv.coords, \n",
    "                        dims=s1_data.vv.dims, attrs=s1_data.vv.attrs)\n",
    "s1_water = s1_water.where(s1_clean_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentinel-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Acquire the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2a_data = dc.load(**common_load_params,\n",
    "                   product='s2a_msil2a', \n",
    "                   time=(s2_times[0], s2_times[-1]),\n",
    "                   dask_chunks = {'time': 1})\n",
    "s2b_data = dc.load(**common_load_params,\n",
    "                   product='s2b_msil2a', \n",
    "                   time=(s2_times[0], s2_times[-1]),\n",
    "                   dask_chunks = {'time': 1})\n",
    "s2_data = xr.concat((s2a_data, s2b_data), dim='time').sortby('time')\n",
    "s2_data = s2_data.sel(time=corresponding_times['Sentinel-2'], method='nearest')\n",
    "print(f\"Subsetting the data to {len(s2_data.time)} times of near coincidence.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Acquire the clean mask**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See figure 3 on this page for more information about the\n",
    "# values of the scl data for Sentinel-2: \n",
    "# https://earth.esa.int/web/sentinel/technical-guides/sentinel-2-msi/level-2a/algorithm\n",
    "s2_clean_mask = s2_data.scl.isin([1, 2, 3, 4, 5, 6, 7, 10, 11]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Acquire water classifications**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"ignore\", category=Warning)\n",
    "    s2_water = wofs_classify(s2_data.rename(\n",
    "        {'nir_1': 'nir', 'swir_1': 'swir1', 'swir_2': 'swir2'})).wofs\n",
    "s2_water = s2_water.where(s2_clean_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls8_data = ls8_data.compute()\n",
    "ls8_clean_mask = ls8_clean_mask.compute()\n",
    "\n",
    "s1_data = s1_data.compute()\n",
    "s1_clean_mask = s1_clean_mask.compute()\n",
    "\n",
    "s2_data = s2_data.compute()\n",
    "s2_clean_mask = s2_clean_mask.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show the RGB Representation of Time Slices and Water Classifications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Obtain the intersected clean mask for the sensors.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intersected_clean_mask = xr.DataArray((ls8_clean_mask.values & \n",
    "                                       s1_clean_mask.values & \n",
    "                                       s2_clean_mask.values), \n",
    "                                       coords=ls8_clean_mask.coords, \n",
    "                                       dims=ls8_clean_mask.dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mask the water classes.\n",
    "ls8_water = ls8_water.where(intersected_clean_mask.values)\n",
    "s1_water = s1_water.where(intersected_clean_mask.values)\n",
    "s2_water = s2_water.where(intersected_clean_mask.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove any times with no data for any sensor.\n",
    "times_to_keep_mask = (intersected_clean_mask.sum(['latitude', 'longitude']) / \\\n",
    "    intersected_clean_mask.count(['latitude', 'longitude'])) > MIN_PCT_DATA\n",
    "# The time indices to keep for visualization.\n",
    "time_inds_subset = np.arange(len(ls8_data.time))[times_to_keep_mask.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intersected_clean_mask_subset = \\\n",
    "    intersected_clean_mask.isel(time=time_inds_subset)\n",
    "\n",
    "ls8_data_subset = ls8_data.isel(time=time_inds_subset)\n",
    "ls8_clean_mask_subset = ls8_clean_mask.isel(time=time_inds_subset)\n",
    "ls8_water_subset = ls8_water.isel(time=time_inds_subset)\n",
    "\n",
    "s1_data_subset = s1_data.isel(time=time_inds_subset)\n",
    "s1_clean_mask_subset = s1_clean_mask.isel(time=time_inds_subset)\n",
    "s1_water_subset = s1_water.isel(time=time_inds_subset)\n",
    "\n",
    "s2_data_subset = s2_data.isel(time=time_inds_subset)\n",
    "s2_clean_mask_subset = s2_clean_mask.isel(time=time_inds_subset)\n",
    "s2_water_subset = s2_water.isel(time=time_inds_subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Show the data and water classifications for each sensor as the data will be compared among them (an intersection).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "water_alpha = 0.9\n",
    "\n",
    "for time_ind in range(len(ls8_data_subset.time)):\n",
    "    fig, ax = plt.subplots(1, 3, figsize=(12, 4))\n",
    "    \n",
    "    # Mask out the water from the RGB so that its background segment is white instead of the RGB.\n",
    "    ls8_data_subset.where(ls8_water_subset != 1)[['red', 'green', 'blue']].isel(time=time_ind).to_array().plot.imshow(ax=ax[0], vmin=0, vmax=1750)\n",
    "    ls8_only_water = ls8_water_subset.where(ls8_water_subset == 1)\n",
    "    ls8_only_water.isel(time=time_ind).plot.imshow(ax=ax[0], cmap='Blues', alpha=water_alpha, \n",
    "                                                   vmin=0, vmax=1, add_colorbar=False)\n",
    "    ax[0].set_xlabel('Longitude')\n",
    "    ax[0].set_ylabel('Latitude')\n",
    "    ax[0].set_title(f\"Landsat 8 \" \\\n",
    "                    f\"({numpy.datetime_as_string(ls8_data_subset.time.values[time_ind], unit='D')})\")\n",
    "    \n",
    "    s1_data_subset.where(s1_water_subset != 1).vv.isel(time=time_ind).plot.imshow(ax=ax[1], cmap='gray', vmin=-30, vmax=-0, add_colorbar=False)\n",
    "    s1_only_water = s1_water_subset.where(s1_water_subset == 1)\n",
    "    s1_only_water.isel(time=time_ind).plot.imshow(ax=ax[1], cmap='Blues', alpha=water_alpha, \n",
    "                                                  vmin=0, vmax=1, add_colorbar=False)\n",
    "    ax[1].set_xlabel('Longitude')\n",
    "    ax[1].set_ylabel('Latitude')\n",
    "    ax[1].set_title(f\"Sentinel-1 \" \\\n",
    "                    f\"({numpy.datetime_as_string(s1_data_subset.time.values[time_ind], unit='D')})\")\n",
    "    \n",
    "    s2_data_subset.where(s2_water_subset != 1)[['red', 'green', 'blue']].isel(time=time_ind).to_array().plot.imshow(ax=ax[2], vmin=0, vmax=2500)\n",
    "    s2_only_water = s2_water_subset.where(s2_water_subset == 1)\n",
    "    s2_only_water.isel(time=time_ind).plot.imshow(ax=ax[2], cmap='Blues', alpha=water_alpha, \n",
    "                                                  vmin=0, vmax=1, add_colorbar=False)\n",
    "    ax[2].set_xlabel('Longitude')\n",
    "    ax[2].set_ylabel('Latitude')\n",
    "    ax[2].set_title(f\"Sentinel-2 \" \\\n",
    "                    f\"({numpy.datetime_as_string(s2_data_subset.time.values[time_ind], unit='D')})\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show the per-time-slice percent of water according to each sensor as a line plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls8_water_subset_pct = \\\n",
    "    ls8_water_subset.sum(['latitude', 'longitude']) / \\\n",
    "    ls8_water_subset.count(['latitude', 'longitude']).compute()\n",
    "\n",
    "s1_water_subset_pct = \\\n",
    "    s1_water_subset.sum(['latitude', 'longitude']) / \\\n",
    "    s1_water_subset.count(['latitude', 'longitude']).compute()\n",
    "s1_water_subset_pct.time.values = ls8_water_subset_pct.time.values\n",
    "\n",
    "s2_water_subset_pct = \\\n",
    "    s2_water_subset.sum(['latitude', 'longitude']) / \\\n",
    "    s2_water_subset.count(['latitude', 'longitude']).compute()\n",
    "s2_water_subset_pct.time.values = ls8_water_subset_pct.time.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.ticker as mtick\n",
    "\n",
    "ax = plt.gca()\n",
    "\n",
    "plot_format = dict(ms=6, marker='o', alpha=0.5)\n",
    "\n",
    "(ls8_water_subset_pct*100).plot(ax=ax, **plot_format, label='Landsat 8')\n",
    "(s1_water_subset_pct*100).plot(ax=ax, **plot_format, label='Sentinel-1')\n",
    "(s2_water_subset_pct*100).plot(ax=ax, **plot_format, label='Sentinel-2')\n",
    "\n",
    "plt.ylim(0,50)\n",
    "\n",
    "ax.set_xlabel('Time')\n",
    "ax.set_ylabel('Percent of Intersecting Data That Is Water')\n",
    "ax.yaxis.set_major_formatter(mtick.PercentFormatter())\n",
    "plt.legend()\n",
    "plt.title('Water %')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show the per-time-slice similarity (% of matching pixels) of each pair of sensors as a line plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "ax = plt.gca()\n",
    "\n",
    "water_das = [('Landsat_8', ls8_water_subset), \n",
    "             ('Sentinel-1', s1_water_subset), \n",
    "             ('Sentinel-2', s2_water_subset)]\n",
    "for i, ((sensor_1, water_1), (sensor_2, water_2)) in enumerate(combinations(water_das, 2)):\n",
    "    lat_dim_ind = np.argmax(np.array(water_1.dims) == 'latitude')\n",
    "    lon_dim_ind = np.argmax(np.array(water_1.dims) == 'longitude')\n",
    "    \n",
    "    similarity = (water_1.values == water_2.values).sum(axis=(lat_dim_ind, lon_dim_ind)) / \\\n",
    "        intersected_clean_mask_subset.sum(['latitude', 'longitude'])\n",
    "    (similarity*100).plot.line(ax=ax, **plot_format, label=f'{sensor_1} vs {sensor_2}')\n",
    "    \n",
    "    ax.set_xlabel('Time')\n",
    "    ax.set_ylabel('Percent of Same Classifications')\n",
    "    ax.yaxis.set_major_formatter(mtick.PercentFormatter())\n",
    "    plt.legend()\n",
    "    plt.title('Similarity')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "7073445e337f46189cb5f163631ca8d5": {
      "model_module": "jupyter-leaflet",
      "model_module_version": "^0.11.1",
      "model_name": "LeafletAttributionControlModel",
      "state": {
       "_model_module_version": "^0.11.1",
       "_view_count": null,
       "_view_module_version": "^0.11.1",
       "options": [
        "position",
        "prefix"
       ],
       "position": "bottomright",
       "prefix": "Leaflet"
      }
     },
     "908a9dc9117c40a58c0270b0ac5c49f4": {
      "model_module": "jupyter-leaflet",
      "model_module_version": "^0.11.1",
      "model_name": "LeafletZoomControlModel",
      "state": {
       "_model_module_version": "^0.11.1",
       "_view_count": null,
       "_view_module_version": "^0.11.1",
       "options": [
        "position",
        "zoom_in_text",
        "zoom_in_title",
        "zoom_out_text",
        "zoom_out_title"
       ]
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
